{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rfClassifier\n",
    "\n",
    "The following changes have been made to Calvin's original code: \n",
    "- Addition of `SMOTE`\n",
    "- Switch to `GradientBoostingClassifier`\n",
    "- Parameter tuning using `RandomizedSearchCV`\n",
    "- Additional metrics summarized and/or plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, learning_curve, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Confusion matrix saved to {save_path}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot roc curve\n",
    "def plot_roc_curve(y_true, y_pred_proba, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save ROC curve.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_true, y_pred_proba[:, 1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"ROC curve saved to {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision recall curve\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save Precision-Recall curve.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_true, y_pred_proba[:, 1])\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Precision-Recall curve saved to {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "def plot_learning_curves(estimator, X, y, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save learning curves.\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=5, n_jobs=-1, \n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='f1'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_sizes, train_mean, label='Training score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1)\n",
    "    plt.plot(train_sizes, val_mean, label='Cross-validation score')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1)\n",
    "    plt.xlabel('Training Examples')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Learning curves saved to {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance\n",
    "def plot_feature_importance(feature_importance, feature_names, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save feature importance.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(x='importance', y='feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 Most Important Features')\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"Feature importance plot saved to {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cross-validation scores\n",
    "def plot_cv_scores(cv_scores, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot and save cross-validation scores distribution.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(x=cv_scores)\n",
    "    plt.title('Cross-validation Scores Distribution')\n",
    "    plt.xlabel('F1 Score')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        print(f\"CV scores distribution saved to {save_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def load_data(data_file):\n",
    "    '''\n",
    "    load_data()\n",
    "\n",
    "    Loads data from CSV file.\n",
    "    \n",
    "    --------\n",
    "    Args:\n",
    "\n",
    "    data_file: (Type: String) Name of CSV file containing data\n",
    "    --------\n",
    "    Returns:\n",
    "\n",
    "    features: (Type: numpy.ndarray) Features of the dataset.\n",
    "    labels: (Type: numpy.ndarray) Labels of the dataset\n",
    "    feature_names: (Type: list) Names of the features\n",
    "    '''\n",
    "    print(f\"Loading data from {data_file}...\")\n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(data_file)\n",
    "\n",
    "    # Extract feature names from the first row\n",
    "    feature_names = df.columns.tolist()[1:]  # Skip the 'Label' column\n",
    "    print(f\"\\nFeature names: {feature_names[:5]}... (total: {len(feature_names)} features)\")\n",
    "    \n",
    "    # First column is the target variable, features are all other columns\n",
    "    y = df.iloc[:, 0].values    # First column\n",
    "    X = df.iloc[:, 1:].values   # All columns except the first one\n",
    "    \n",
    "    print(f\"Loaded {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "    print(\"Class distribution before SMOTE:\")\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"Class {label}: {count} samples\")\n",
    "    \n",
    "    return X, y, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model parameters\n",
    "def save_results(results, filename=None):\n",
    "    '''\n",
    "    Save model results to a JSON file.\n",
    "    \n",
    "    --------\n",
    "    Args:\n",
    "    results: (Type: dict) Dictionary containing model results\n",
    "    filename: (Type: str) Optional filename, defaults to timestamp\n",
    "    '''\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"model_results_{timestamp}.json\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(f\"Results saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set Random Seed for Reproducability\n",
    "    np.random.seed(89)\n",
    "\n",
    "    # Load Data\n",
    "    X, y, feature_names = load_data(file)\n",
    "\n",
    "    # Generate train and test splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=89, stratify=y)\n",
    "    print(f\"\\nTraining set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # Check the number of minority class samples in the training set\n",
    "    minority_class_count = np.sum(y_train == 1)\n",
    "    print(f\"Number of minority class samples in training set: {minority_class_count}\")\n",
    "    \n",
    "    # Determine a safe k_neighbors value for SMOTE\n",
    "    # We need to ensure k_neighbors is less than the number of minority class samples\n",
    "    safe_k = max(1, min(3, minority_class_count - 1))\n",
    "    print(f\"Using k_neighbors={safe_k} for SMOTE to avoid errors\")\n",
    "\n",
    "    # Define the pipeline with SMOTE inside the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('smote', SMOTE(random_state=89, k_neighbors=safe_k, sampling_strategy='auto')),\n",
    "        ('classifier', GradientBoostingClassifier(random_state=89))\n",
    "    ])\n",
    "\n",
    "    # Define parameter distributions for RandomizedSearchCV\n",
    "    # Use a smaller range for k_neighbors to avoid errors\n",
    "    # Add regularization parameters to reduce overfitting\n",
    "    param_distributions = {\n",
    "        'smote__k_neighbors': [1, 2, 3],  # More k_neighbors options\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],  # More estimators\n",
    "        'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],  # More learning rates\n",
    "        'classifier__max_depth': [2, 3, 4],  # More depth options\n",
    "        'classifier__min_samples_split': [2, 5, 10, 15, 20],  # More split options\n",
    "        'classifier__min_samples_leaf': [2, 4, 6, 8],  # More leaf options\n",
    "        'classifier__subsample': [0.6, 0.7, 0.8, 0.9, 1.0],  # More subsample options\n",
    "        'classifier__max_features': ['sqrt', 'log2', None]  # Added None option\n",
    "    }\n",
    "\n",
    "    # Create RandomizedSearchCV object with stratification\n",
    "    random_search = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        param_distributions,\n",
    "        n_iter=50,  # Increased number of iterations\n",
    "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=89),  # Increased number of folds\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        error_score='raise'\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV with progress bar\n",
    "    print(\"\\nStarting RandomizedSearchCV...\")\n",
    "    with tqdm(total=100, desc=\"Training Progress\") as pbar:\n",
    "        random_search.fit(X_train, y_train)\n",
    "        pbar.update(100)\n",
    "\n",
    "    # Get cross-validation scores with more folds\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=89)\n",
    "    cv_scores = cross_val_score(random_search.best_estimator_, X_train, y_train, cv=cv, scoring='f1')\n",
    "    print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "    # Get the resampled training data for visualization\n",
    "    best_pipeline = random_search.best_estimator_\n",
    "    X_train_resampled, y_train_resampled = best_pipeline.named_steps['smote'].fit_resample(\n",
    "        best_pipeline.named_steps['scaler'].fit_transform(X_train), \n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    # Collect results\n",
    "    results = {\n",
    "        'best_parameters': random_search.best_params_,\n",
    "        'best_cv_score': float(random_search.best_score_),\n",
    "        'cv_scores': cv_scores.tolist(),\n",
    "        'training_time': time.time() - start_time,\n",
    "        'n_samples': X.shape[0],\n",
    "        'n_features': X.shape[1],\n",
    "        'class_distribution_before': {str(k): int(v) for k, v in zip(*np.unique(y, return_counts=True))},\n",
    "        'class_distribution_after_smote': {str(k): int(v) for k, v in zip(*np.unique(y_train_resampled, return_counts=True))},\n",
    "        'best_k_neighbors': random_search.best_params_['smote__k_neighbors']\n",
    "    }\n",
    "\n",
    "    # Get best model\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Calculate test metrics\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    results['test_accuracy'] = float(metrics.accuracy_score(y_test, y_pred))\n",
    "    results['test_auc'] = float(metrics.roc_auc_score(y_test, y_pred_proba[:, 1]))\n",
    "    results['test_f1'] = float(metrics.f1_score(y_test, y_pred))\n",
    "    results['test_precision'] = float(metrics.precision_score(y_test, y_pred))\n",
    "    results['test_recall'] = float(metrics.recall_score(y_test, y_pred))\n",
    "    results['classification_report'] = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== Model Performance ===\")\n",
    "    print(f\"Best parameters: {results['best_parameters']}\")\n",
    "    print(f\"Best CV score: {results['best_cv_score']:.4f}\")\n",
    "    print(f\"Test accuracy: {results['test_accuracy']:.4f}\")\n",
    "    print(f\"Test AUC: {results['test_auc']:.4f}\")\n",
    "    print(f\"Test F1-score: {results['test_f1']:.4f}\")\n",
    "    print(f\"Test Precision: {results['test_precision']:.4f}\")\n",
    "    print(f\"Test Recall: {results['test_recall']:.4f}\")\n",
    "    print(f\"\\nTraining time: {results['training_time']:.2f} seconds\")\n",
    "\n",
    "    # Generate and save visualizations\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_confusion_matrix(y_test, y_pred, f'confusion_matrix_{timestamp}.png')\n",
    "    plot_roc_curve(y_test, y_pred_proba, f'roc_curve_{timestamp}.png')\n",
    "    plot_precision_recall_curve(y_test, y_pred_proba, f'precision_recall_curve_{timestamp}.png')\n",
    "    plot_learning_curves(best_model, X_train, y_train, f'learning_curves_{timestamp}.png')\n",
    "    plot_cv_scores(cv_scores, f'cv_scores_{timestamp}.png')\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = best_model.named_steps['classifier'].feature_importances_\n",
    "    plot_feature_importance(feature_importance, feature_names, f'feature_importance_{timestamp}.png')\n",
    "    \n",
    "    features_rank = pd.Series(feature_importance, index=feature_names).sort_values(ascending=False)\n",
    "    print(\"\\n=== Top 10 Most Important Features ===\")\n",
    "    print(features_rank.head(400))\n",
    "\n",
    "    # Save results\n",
    "    save_results(results)\n",
    "\n",
    "    return best_model, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from synthetic_data.csv...\n",
      "\n",
      "Feature names: ['FP_r_Metric01', 'FP_l_Metric01', 'IC_r_Metric01', 'IC_l_Metric01', 'SFG_r_Metric01']... (total: 792 features)\n",
      "Loaded 400 samples with 792 features\n",
      "Class distribution before SMOTE:\n",
      "Class 0: 372 samples\n",
      "Class 1: 28 samples\n",
      "\n",
      "Training set size: 320, Test set size: 80\n",
      "Number of minority class samples in training set: 22\n",
      "Using k_neighbors=3 for SMOTE to avoid errors\n",
      "\n",
      "Starting RandomizedSearchCV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "# run main() and store the returned values\n",
    "best_model, feature_names = main(\"synthetic_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model pipeline (which includes SMOTE + scaler + classifier)\n",
    "joblib.dump(pipeline, \"model.pkl\")\n",
    "\n",
    "# Save the test set used for evaluation (optional)\n",
    "X_test.to_csv(\"X_test.csv\", index=False)\n",
    "y_test.to_csv(\"y_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
